\chapter{GENERAL BACKGROUND \& REVIEW OF LITERATURE}

\subsection{What are Grammars?}

Grammars in computer science can be defined as a set of rules by which valid sentences in a language are constructed \cite{jiangFormalGrammarsLanguages}, serving as a blueprint for the language. Beginning with a start symbol, which is a single non-terminal, production rules are applied sequentially, adding symbols from the alphabet according to the grammar’s production rules, to derive a string that is valid in the language \cite{GrammarTheoryComputation2025}.

\vspace{\baselineskip}

A grammar can be represented by the tuple $<N, T, P, S>$, where:
\begin{itemize}
\item $N$ is a finite, non-empty set of non-terminal symbols/alphabet.
\item $T$ is a finite set of terminal symbols/alphabet.
\item $P$ is a finite set of production rules.
\item $S$ is the start symbol (a non-terminal symbol/alphabet).
\end{itemize}

Here, the production rules $P$ define all symbol substitutions that can be performed recursively to generate different symbol sequences, known as \emph{strings} \cite{GrammarTheoryComputation2025}. These rules are written in the form $A \rightarrow w$, where $A \in N$ and $w \in (N \cup T)^*$.

\vspace{\baselineskip}
These sets of rules are used by parsers to parse a string of tokens and analyse its syntax against this set of rules(Grammars)\cite{mulikComparisonParsingTechniques,hendriksConsiderItParsed}.

\subsection{What are Context-Free Grammars?}

Depending on the set of production rules, each grammar can be classified according to the \emph{Chomsky hierarchy} \cite{chomskyTHREEMODELSTIE1956}.

\begin{center}
    \includegraphics[width=0.7\textwidth]{chapter1/ch_heir.png}
    \captionof{figure}{Chomsky hierarchy \cite{ChomskyHierarchyTheory2025}}
    \label{fig:hierarchy}
\end{center}

The hierarchy outlines four distinct types of grammars, ranging from \textbf{Type-3} (the most restrictive) to \textbf{Type-0} (the most general and unstructured). \textbf{Type-2} grammars in this hierarchy are known as \emph{context-free grammars}.

\subsubsection{Type-3}
Type-3 grammars, known as regular grammars, are used to generate regular languages. A grammar is classified as a regular grammar if its production rules follow from $X \rightarrow a | X \rightarrow aY$. The left-hand side must consist of a single $T$ and the right-hand side must consist of a single $T$ or a single $T$ and a single $N$.

Regular grammar can take 2 forms, right linear and left linear.

Right-linear takes the following form, where the $N$ on the right-hand side of the arrow is on the far right.
\begin{itemize}
    \item[] $X \rightarrow a$
    \item[] $X \rightarrow aB$
    \item[] $X \rightarrow \varepsilon$
\end{itemize}

Left-linear take the following form, where the $N$ on the right-hand side of the arrow is on the far left.
\begin{itemize}
    \item[] $X \rightarrow a$
    \item[] $X \rightarrow Ba$
    \item[] $X \rightarrow \varepsilon$
\end{itemize}

Note that right-linear and left-linear forms cannot be mixed, 
Doing so may generate languages that are not regular. 
Additionally, production of $\varepsilon$ is allowed only if the corresponding nonterminal 
does not appear on the right-hand side of any production rule \cite{hendriksConsiderItParsed,shiIntelligenceScience2021}

\subsubsection{Type-2}

Type-2 grammars, commonly known as context-free grammars (CFGs), have production rules of the form:
\[
A \rightarrow \alpha
\]
where \( A \in N \) and \( \alpha \in (T \cup N)^* \), meaning any string composed of terminal and nonterminal symbols \cite{hendriksConsiderItParsed,shiIntelligenceScience2021}.

Due to the nature of the right-hand side, nonterminals are allowed to recursively expand, which can lead to repetition. For example:
\begin{itemize}
    \item[] \( A \rightarrow sAb \)
    \item[] \( A \rightarrow \varepsilon \)
\end{itemize}
can produce derivations of the form
\[
A \Rightarrow sAb \Rightarrow ssAbb \Rightarrow \cdots \Rightarrow s^n b^n,
\]
until eventually \( A \Rightarrow \varepsilon \).

This property is particularly useful because it allows the grammar to enforce well-formed parentheses expressions and other recursive structures purely through production rules. As a result, context-free grammars are of great importance in the design and specification of programming languages\cite{hendriksConsiderItParsed,shiIntelligenceScience2021}.

\subsubsection{Type-1}

Type-1 grammars, also known as context-sensitive grammars, have production rules of the form:
\[
\alpha A \beta \rightarrow \alpha \gamma \beta
\]
where \( A \in N \) and \( \alpha, \beta, \gamma \in (N \cup T)^* \). This means that \( A \) can be expanded to \( \gamma \) only in the specific context where it appears between \( \alpha \) and \( \beta \) in the given order \cite{hendriksConsiderItParsed,shiIntelligenceScience2021}.

\vspace{\baselineskip}
A key property of context-sensitive grammars is that production rules must be \emph{non-contracting}: the length of the right-hand side must be greater than or equal to the length of the left-hand side. This implies that no nonterminal on the right-hand side can derive \( \varepsilon \), meaning nullable productions are not allowed.

An example derivation using a context-sensitive production is:
\[
A \Rightarrow aA\beta \Rightarrow aaA\beta\beta \Rightarrow \cdots
\]

\subsubsection{Type-0}

Type-0 grammars, also known as unrestricted grammars, sit at the top of Chomsky's hierarchy. Their production has a complete lack of restrictions and takes the general form:
\[
\alpha \rightarrow \beta
\]
where \( \alpha, \beta \in (N \cup T)^+ \), meaning they are strings consisting of terminal and nonterminal symbols. The only requirement is that \( \alpha \) must contain at least one nonterminal symbol to allow for further derivations \cite{hendriksConsiderItParsed,shiIntelligenceScience2021}.

Type-0 grammars are the most expressive class in the Chomsky hierarchy and can generate all recursively enumerable languages.

\subsection{What is ARVADA?}

\subsubsection{Introduction}

ARVADA is an algorithm published in \enquote{Learning Highly Recursive Input Grammars} \cite{kulkarniLearningHighlyRecursive2021} at the University of California, Berkeley in 2021. It is designed to learn context-free grammars from a set of positive examples and a Boolean-valued oracle $\mathcal{O}$. Starting from initially flat parse trees, ARVADA repeatedly applies two specialised operations, \textbf{bubbling} and \textbf{merging}, to incrementally add structure to these trees. From this structured representation, it extracts the smallest possible set of context-free grammar rules that accommodate all the given examples. The algorithm aims to generalise the language as much as possible without overgeneralising beyond what is accepted by $\mathcal{O}$.

\vspace{\baselineskip}
Like GLADE \cite{bastaniSynthesizingProgramInput}, ARVADA operates under the assumption of a black-box oracle $\mathcal{O}$. This means that ARVADA has no access to or knowledge of the internal workings of the oracle and can only observe the Boolean values returned by $\mathcal{O}$.

\subsubsection{GLADE}
GLADE is an algorithm proposed by Bastani et al., published in Synthesising Input Grammars at PLDI 2017 \cite{bastaniSynthesizingProgramInput}. Like ARVADA, GLADE uses a set of valid inputs and black-box access to the program, with the aim of automatically approximating the context-free input grammar of the given program.

\vspace{\baselineskip}

Because GLADE shares a similar experimental setting as ARVADA and is a predecessor, GLADE was used as a benchmark for ARVADA during its evaluation. It was shown that GLADE, on average, had a faster runtime compared to ARVADA. However, in terms of generalisation, following the original grammar of the oracle more closely, ARVADA outperformed GLADE across the 11 benchmarks, achieving a higher F1 score on 9 of the 11 benchmarks.

\subsubsection{Explanation}

ARVADA takes as input the oracle $\mathcal{O}$ and a set of positive, valid oracle inputs $S$. For each string $s \in S$, querying $\mathcal{O}(s)$ returns \verb|True|. The algorithm begins by constructing a flat parse tree for each string in $S$. Each tree has a single root node $t_0$ whose children correspond to the individual characters of the input string $s$.

\vspace{\baselineskip}
Next, ARVADA performs the \textbf{bubbling} operation. In this step, a sequence of sibling nodes in the tree is selected and replaced with a new non-terminal node. This new node takes the selected sibling nodes as its children, thereby introducing an additional level of structure. Essentially, ARVADA transforms sequences of terminal nodes in the flat parse tree into subtrees by introducing new non-terminal nodes and progressively adding structure to the tree.

\vspace{\baselineskip}
ARVADA then decides whether to accept or reject each bubble by checking whether the newly bubbled structure enables a sound generalisation of the learned grammar. Each non-leaf node in the tree can be viewed as a non-terminal in the emerging grammar. To determine whether a bubble should be accepted, ARVADA checks whether replacing any node in the tree with the new bubbled subtree results in the generation of valid input strings according to $\mathcal{O}$. If the replacement produces valid strings, the bubble is accepted, and the tree is restructured so that both the bubbled subtree and the replaced node share the same non-terminal label.

\vspace{\baselineskip}
The addition of new non-terminal nodes expands the language defined by the learned grammar, since any string derivable from the same label can now be substituted interchangeably. This relabeling of the bubbled subtree and the replaced node is called a \textbf{merge}, as it merges the labels of two previously distinct nodes in the tree. If a bubble is not accepted, it is discarded, and none of the trees are affected or structurally modified.

\subsubsection{Walkthrough}

This walkthrough will follow very closely to the examples provided in the original paper \cite{kulkarniLearningHighlyRecursive2021}, and use a concrete example to provide an in-depth understanding of ARVADA.

%%%%%%%%% GRAMMAR %%%%%%%%%%%
\begin{figure}[H]
\begin{tcolorbox}[title=$G_w$, colback=white, colframe=black]
\begin{grammar}{
    \pr{\emph{start}}{\emph{stmt}}
    
    \pr{\emph{stmt}}{while\textvisiblespace\ \emph{boolexpr}\textvisiblespace\ do\textvisiblespace\ \emph{stmt}}
    & & \gors if\textvisiblespace\ \emph{boolexpr}\textvisiblespace\ then\textvisiblespace\ \emph{stmt}\textvisiblespace\ else\textvisiblespace\ \emph{stmt}\\
    & & \gors L\textvisiblespace\ =\textvisiblespace\ \emph{numexpr}\\
    & & \gors \emph{stmt}\textvisiblespace\ ;\textvisiblespace\ \emph{stmt}\\
    
    \pr{\emph{boolexpr}}{$\thicksim$\emph{boolexpr}}
    & & \gors \emph{boolexpr}\textvisiblespace\ \&\textvisiblespace\ \emph{boolexpr}\\
    & & \gors \emph{numexpr}\textvisiblespace\ ==\textvisiblespace\ \emph{numexpr}\\
    & & \gors false\\
    & & \gors true\\
    
    \pr{\emph{numexpr}}{(\textvisiblespace\ \emph{numexpr}\textvisiblespace\ +\textvisiblespace\ \emph{numexpr}\textvisiblespace\ )}
    & & \gors L\\
    & & \gors n\\
}
\end{grammar}
\end{tcolorbox}

\[
S = \left\{
\text{\texttt{while true \& false do L = n}},
\quad
\text{\texttt{L = n ; L = (n+n)}}
\right\}
\]

\[
O(i) =
\begin{cases}
\text{True} & \text{if } i \in \mathcal{L}(G_w) \\
\text{False} & \text{otherwise}
\end{cases}
\]
\caption{Definition a simple while grammar $G_w$, sample input strings $S$, and oracle $\mathcal{O}$ \cite{kulkarniLearningHighlyRecursive2021}}
\label{fig:grammar}
\end{figure}
%%%%%%%%%%%%%%%%%%%

%%% proofread here please %%%%%
For this walkthrough, the simple while grammar $G_w$ and input strings $S$ provided in figure~\ref{fig: grammar} will be used. Clarifying again that ARVADA treats $\mathcal{O}$ as a black box, meaning it has no structural knowledge of $G_w$, and $G_w$ is only shown to clarify the behaviour of $\mathcal{O}$ for understanding and comprehension.

%%%%% Pseudocode $$$$$$
\begin{algorithm}[H]
\caption{High-level overview of ARVADA \cite{kulkarniLearningHighlyRecursive2021}}\label{alg:arv}
\begin{algorithmic}[1]
    \Require a set of examples $S$, a language oracle $\mathcal{O}$.
    \State $bestTrees \gets \textsc{NaiveParseTrees}(S)$
    \State $bestTrees \gets \textsc{MergeAllValid}(bestTrees, \mathcal{O})$
    \State $updated \gets \text{True}$
    \While{$updated$}
        \State $updated \gets \text{False}$
        \State $allBubbles \gets \textsc{GetBubbles}(bestTrees)$
        \For{$bubble$ \textbf{in} $allBubbles$}
            \State $bbldTrees \gets \textsc{Apply}(bestTrees, bubble)$
            \State $accepted, mergedTs \gets \textsc{CheckBubble}(bbldTrees, \mathcal{O})$
            \If{$accepted$}
                \State $bestTrees \gets mergedTs$
                \State $updated \gets \text{True}$
                \State \textbf{break}
            \EndIf
        \EndFor
    \EndWhile
    \State $G \gets \textsc{InducedGrammar}(bestTrees)$
    \State \textbf{Return} $G$
\end{algorithmic}
\end{algorithm}

ARVADA follows the high-level overview provided in algorithm ~\ref{alg:arv}, and begins by taking all the input strings in $S$ and building a naive flat parse tree for each input string.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
    \node {$t_0$}
        child {node {w}}
        child {node {h}}
        child {node {i}}
        child {node {l}}
        child {node {e}}
        child {node {\textvisiblespace}}
        child {node {t}}
        child {node {r}}
        child {node {u}}
        child {node {e}}
        child {node {\textvisiblespace}}
        child {node {\&}}
        child {node {\textvisiblespace}}
        child {node {f}}
        child {node {a}}
        child {node {l}}
        child {node {s}}
        child {node {e}}
        child {node {\textvisiblespace}}
        child {node {d}}
        child {node {o}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}

\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
\node {$t_0$}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {;}}
    child {node {\textvisiblespace}}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node {(}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {+}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {)}};    
\end{tikzpicture}
\caption{Initial set of flat naive parse trees ARVADA builds given inputs $S$, where each $t_i$ is a non-terminal}
\label{fig:initialNaiveTrees}
\end{figure}

In this initially naive parse tree, for each string input. There is a non-terminal $t_0$, and all characters in the string, including the spaces, are child nodes. From this alone, the following grammar can be induced.

\[
t_0 \to \text{w\;h\;i\;l\;e\;\textvisiblespace\;t\;r\;u\;e\textvisiblespace\&\textvisiblespace\;f\;a\;l\;s\;e\;\textvisiblespace\;d\;o\;\textvisiblespace\;L\;\textvisiblespace=\;\textvisiblespace\;n}
\]
\[
t_0 \to \text{L\;\textvisiblespace\;=\;\textvisiblespace \;n\;\textvisiblespace\;;\;\textvisiblespace\;L\;\textvisiblespace\;=\;\textvisiblespace\;(\;n\;+\;n\;)}
\]

The string derivable from a node $N$, in this case $t_0$, is the concatenation of all its leaf nodes or itself, if $N$ happens to be a leaf node. 

\vspace{\baselineskip}
Next, although not present in pseudocode \ref{alg:arv}, ARVADA has a functionality called pre-tokenisation, which can be toggled on or off. Pre-tokenisation is a step after all the naive trees are built, where the algorithm groups together sequences of contiguous characters of the same class (lowercase, uppercase, whitespaces, digits, etc) into leaf tokens and all punctuations are kept separate.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=7mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
    \node {$t_0$}
        child {node {while}}
        child {node {\textvisiblespace}}
        child {node {true}}
        child {node {\textvisiblespace}}
        child {node {\&}}
        child {node {\textvisiblespace}}
        child {node {false}}
        child {node {\textvisiblespace}}
        child {node {do}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}

\caption{ARVADA pre-tokenisations example}
\label{fig:tokenisedNaiveTrees}
\end{figure}

Next, ARVADA attempts its first optimisation, performing MERGEALLVALID, which attempts to generalise and add structure to the tree by seeing if any of the leaf nodes/tokens can already be merged and put under new non-terminal labels. MERGEALLVALID looks at 2 leaf nodes if it is non-tokenised and/or 2 non-terminals if tokenised at a time, let these be $t_a$ and $t_b$. Then it goes through all the parse trees, creating a replica of each tree with all instances of $t_b$ replaced with $t_a$. Let the original parse trees be $T$ and the replica with replacements be $T'$. From the replica, new candidate strings can be derived; these candidate strings are fed into $\mathcal{O}$. The same process is done, where all instances of $t_a$ are replaced with $t_b$. Now having 2 sets of candidate strings, where in 1 set, $t_b$ replaces $t_a$, and other set, $t_a$ replaces $t_b$. If all the strings in both sets are accepted by $\mathcal{O}$, then $t_a$ and $t_b$ are put under a new non-terminal labelled $t_c$, and a successful merge has occurred. Note roots $t_0$ are also non-terminal and can be $t_a$ or $t_b$.


\begin{figure}[H]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=7mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
    \node {$t_0$}
        child {node {while}}
        child {node {\textvisiblespace}}
        child {node {true}}
        child {node {\textvisiblespace}}
        child {node {\&}}
        child {node {\textvisiblespace}}
        child {node {false}}
        child {node {\textvisiblespace}}
        child {node {do}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}

\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
\node {$t_0$}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {;}}
    child {node {\textvisiblespace}}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node {(}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {+}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {)}};    
\end{tikzpicture}

$\big\downarrow$

\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=7mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
    \node {$t_0$}
        child {node {while}}
        child {node {\textvisiblespace}}
        child {node[fill=green] {$t_1$}
            child {node {ture}}
        }
        child {node {\textvisiblespace}}
        child {node {\&}}
        child {node {\textvisiblespace}}
        child {node[fill=green] {$t_1$}
            child {node {false}}
        }
        child {node {\textvisiblespace}}
        child {node {do}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}


\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
\node {$t_0$}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node[fill=yellow] {$t_2$}
        child {node {n}}
    }
    child {node {\textvisiblespace}}
    child {node {;}}
    child {node {\textvisiblespace}}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node {(}}
    child {node {\textvisiblespace}}
    child {node[fill=yellow] {$t_2$}
        child {node {n}}
    }
    child {node {\textvisiblespace}}
    child {node {+}}
    child {node {\textvisiblespace}}
    child {node[fill=yellow] {$t_2$}
        child {node {n}}
    }
    child {node {\textvisiblespace}}
    child {node {)}};    
\end{tikzpicture}

\caption{Examples of a $\texttt{MERGEALLVALID}$ run, tokenised and non tokenisation}
\label{fig:mergallvalid}
\end{figure}

As MERGEALLVALID only looked at 1 leaf-node or non-terminal on its own (no sequence of leaf-node and/or non-terminals), ARVADA now moves onto bubbling. Where bubbling now looks at a sequence of leaf-nodes and/or non-terminal nodes. 

\vspace{\baselineskip}
The fundamental next step ARVADA perform is to bubble up a sequence of sibling nodes in the current trees $T$ into a new non-terminal.  To bubble up a sequcne $s_1$ in trees $T$ a  new non-terminal is created ${t_s}_1$ with children $s_1$ and all occurrences of $s_1$ in each tree $t$ in $T$ with ${t_s}_1$. Example shown in figure \ref{fig: possible bubble}, where $t_3$ is a bubbled up. After bubbling up a sequence $s_1$, ARVADA either accepts or rejects the bubble. A bubble is only accepted if the enables a valid generalisation of the example string inputs. Meaning if the relabeling of the bubbled non-terminal, merging its label with the label of another existing node, expands the language accepted by the induced grammar while still remaining valid to $\mathcal{O}$. In practice, merging here is done in a similar fashion to the merging in MERGEALLVALID. Given 2 labels $t_a$ and $t_b$, ARVADA mutates trees from the current trees in $T$ such that subtrees rooted at $t_a$ are replaced by subtrees rooted at $t_b$ and vice versa, which, when concatenated, gives a candidate string which can be tested against the $\mathcal{O}$.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
\node {$t_0$}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node[fill=yellow] {$t_2$}
        child {node {n}}
    }
    child {node {\textvisiblespace}}
    child {node {;}}
    child {node {\textvisiblespace}}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node[fill=green] {$t_3$}
        child {node[fill=green] {(}}
        child {node[fill=green] {\textvisiblespace}}
        child {node[fill=green] {$t_2$}
            child {node[fill=green] {n}}
        }
        child {node[fill=green] {\textvisiblespace}}
        child {node[fill=green] {+}}
        child {node[fill=green] {\textvisiblespace}}
        child {node[fill=green] {$t_2$}
            child {node[fill=green] {n}}
        }
        child {node[fill=green] {\textvisiblespace}}
        child {node[fill=green] {)}}    
    };
\end{tikzpicture}


\caption{Possbile bubble example}
\label{fig: possible bubble}
\end{figure}

Referring to the pseudocode in \ref{alg:arv}, from the current trees $T$ after MERGEALLVALID, ARVADA performs GETBUBBLES. For each tree $t \in T$, GETBUBBLES collects all proper contiguous subsequences of children in $t$. That is, if a tree contains a node $t_i$ with children $C = c_1, c_2,..., c_n$, the potential bubble for this tree includes all sequences of length greater than 1 and less than n. Note that if the children are a subtree, the structure is maintained, like in figure \ref{fig: possible bubble}. GETBUBBLES returns all these as 1-bubble, and all non-conflicting pairs of these as 2-bubbles. Two subsequences are non-conflicting if they do not strictly overlap: they can be disjoint or a proper subsequence of the other. So $((c_1, c_2, c_3), (c_4, c_5, c_6))$ and $((c_1, c_2, c_3), (c_2, c_3))$ are fine, however $((c_1, c_2, c_3), (c_2, c_3, c_4))$ is not. The purpose of 2 bubbles is to account for the limitations of 1-bubble. At some point, ARVADA will reach a stage where no 1-bubble can be merged with an existing label of a node in the tree, but more generalisations can still be made. Consider figure \ref{fig:fullrun2} (3), at this point in the run, ARVADA finds the single bubble $true \rightarrow t_5$ cannot be validly merged with any other existing node in the trees $T$. To cope with this, ARVADA bubbles up 2 distinct sequences (distinct or sub/super sets), only accepting the 2-bubble if they can merge, as seen in the example. $t_6 \rightarrow false$. 
\vspace{\baselineskip}

Now, ARVADA goes through 1-bubbles, taking 1 at a time and attempts to see if it can validly merge the current bubble with another non-terminal in the original trees. This is done in CHECKBUBBLE. If the merge is accepted, the trees are updated to reflect the successful merge; no further bubbles are checked, all current bubbles are discarded, and the process is repeated with the new updated trees. Meaning, new bubbles are formed and checked. 

\vspace{\baselineskip}
ARVADA to optimise checking bubbles that are more likely to get accepted first uses certain heuristics to order the exploration of the bubbles. Primarily sorting by similarity in context first, and frequency second and taking the top 100. Technical details are explained in the implementation.

\vspace{\baselineskip}
After all the bubbles, 1-bubbles and 2-bubbles are exhausted, it looks at the final stage of the parse trees and returns the induced grammar. More technical explanations are provided in the implementation section of this paper.
%%%%%%%%%%% Full run %%%%%%

\begin{figure}[H]
\centering

\begin{tikzpicture}[
    node distance=10mm, every node/.style={align=center}]
    % left tree
    \node (lefttree) {
        \begin{tikzpicture}[
            level distance=10mm,
            sibling distance=4mm,
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {w}}
            child {node {h}}
            child {node {i}}
            child {node {l}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {t}}
            child {node {r}}
            child {node {u}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {\&}}
            child {node {\textvisiblespace}}
            child {node {f}}
            child {node {a}}
            child {node {l}}
            child {node {s}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {d}}
            child {node {o}}
            child {node {\textvisiblespace}}
            child {node {L}}
            child {node {\textvisiblespace}}
            child {node {=}}
            child {node {\textvisiblespace}}
            child {node {n}};
        \end{tikzpicture}
    };

    % right tree (placed in same row)
    \node[right=of lefttree] (righttree) {
        \begin{tikzpicture}[
            level distance=12mm,
            sibling distance=3mm,
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {L}}
            child {node {\textvisiblespace}}
            child {node {=}}
            child {node {\textvisiblespace}}
            child {node {n}}
            child {node {\textvisiblespace}}
            child {node {;}}
            child {node {\textvisiblespace}}
            child {node {L}}
            child {node {\textvisiblespace}}
            child {node {=}}
            child {node {\textvisiblespace}}
            child {node {(}}
            child {node {\textvisiblespace}}
            child {node {n}}
            child {node {\textvisiblespace}}
            child {node {+}}
            child {node {\textvisiblespace}}
            child {node {n}}
            child {node {\textvisiblespace}}
            child {node {)}};
        \end{tikzpicture}
    };
\end{tikzpicture}

$\vert$

(1) Bubble $t_2 \rightarrow$ ( n + n ); merge ($t_2$, n) into $t_3$

$\big\downarrow$

\begin{tikzpicture}[node distance=10mm, every node/.style={align=center}]
    % left tree
    \node (lefttree) {
        \begin{tikzpicture}[
            level distance=10mm,
            sibling distance=4mm,
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {w}}
            child {node {h}}
            child {node {i}}
            child {node {l}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {t}}
            child {node {r}}
            child {node {u}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {\&}}
            child {node {\textvisiblespace}}
            child {node {f}}
            child {node {a}}
            child {node {l}}
            child {node {s}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {d}}
            child {node {o}}
            child {node {\textvisiblespace}}
            child {node {L}}
            child {node {\textvisiblespace}}
            child {node {=}}
            child {node {\textvisiblespace}}
            child {node[fill=orange] {$t_3$}
                child {node {n}}
            };
        \end{tikzpicture}
    };

    % right tree (placed in same row)
    \node[right=of lefttree] (righttree) {
        \begin{tikzpicture}[
            level distance=12mm,
            sibling distance=3mm,
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {L}}
            child {node {\textvisiblespace}}
            child {node {=}}
            child {node {\textvisiblespace}}
            child {node[fill=orange] {$t_3$}
                child {node {n}}
            }
            child {node {\textvisiblespace}}
            child {node {;}}
            child {node {\textvisiblespace}}
            child {node {L}}
            child {node {\textvisiblespace}}
            child {node {=}}
            child {node {\textvisiblespace}}
            child {node[fill=orange] {$t_3$}
                child {node {(}}
                child {node {\textvisiblespace}}
                child {node[fill=orange] {$t_3$}
                    child {node {n}}
                }
                child {node {\textvisiblespace}}
                child {node {+}}
                child {node {\textvisiblespace}}
                child {node[fill=orange] {$t_3$}
                    child {node {n}}
                }
                child {node {\textvisiblespace}}
                child {node {)}}
            };
        \end{tikzpicture}
    };
\end{tikzpicture}

$\vert$

(2) Bubble $t_4 \rightarrow$ L\textvisiblespace=\textvisiblespace $t_3$; merge ($t_4, t_0$) into $t_0$

$\big\downarrow$


\begin{tikzpicture}[node distance=10mm, every node/.style={align=center}]
    % left tree
    \node (lefttree) {
        \begin{tikzpicture}[
            level distance=10mm,
            sibling distance=4mm,
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {w}}
            child {node {h}}
            child {node {i}}
            child {node {l}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {t}}
            child {node {r}}
            child {node {u}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {\&}}
            child {node {\textvisiblespace}}
            child {node {f}}
            child {node {a}}
            child {node {l}}
            child {node {s}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node {d}}
            child {node {o}}
            child {node {\textvisiblespace}}
            child {node[fill=orange] {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {n}}
                }
            };

        \end{tikzpicture}
    };

    % right tree (placed in same row)
    \node[right=of lefttree] (righttree) {
        \begin{tikzpicture}[
            level distance=12mm,
            level 1/.style={sibling distance=5mm}, % children of the root
            level 2/.style={sibling distance=4mm}, % grandchi1
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node[fill=orange] {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {n}}
                }
            }
            child {node {\textvisiblespace}}
            child {node {;}}
            child {node {\textvisiblespace}}
            child {node[fill=orange] {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {(}}
                    child {node {\textvisiblespace}}
                    child {node {$t_3$}
                        child {node {n}}
                    }
                    child {node {\textvisiblespace}}
                    child {node {+}}
                    child {node {\textvisiblespace}}
                    child {node {$t_3$}
                        child {node {n}}
                    }
                    child {node {\textvisiblespace}}
                    child {node {)}}
                }
            };
        \end{tikzpicture}
    };
\end{tikzpicture}


\caption{Example full run of ARVADA bubbling using non-tokenisation nor optimised tree, and the grammar induced (part 1)\cite{kulkarniLearningHighlyRecursive2021}}
\label{fig:fullrun1}
\end{figure}


\begin{figure}[H]
\centering
(3) Bubble $t_5 \rightarrow false, t_6 \rightarrow ture$; merge ($t_5, t_6$) into $t_7$

$\big\downarrow$

\begin{tikzpicture}[node distance=10mm, every node/.style={align=center}]
    % left tree
    \node (lefttree) {
        \begin{tikzpicture}[
            level 1/.style={sibling distance=5mm}, % children of the root
            level 2/.style={sibling distance=4mm}, % grandchi1
            level distance=10mm,
            sibling distance=4mm,
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {w}}
            child {node {h}}
            child {node {i}}
            child {node {l}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node [fill=orange]{$t_7$}
                child {node {t}}
                child {node {r}}
                child {node {u}}
                child {node {e}}
            }
            child {node {\textvisiblespace}}
            child {node {\&}}
            child {node {\textvisiblespace}}
            child {node [fill=orange]{$t_7$}
                child {node {f}}
                child {node {a}}
                child {node {l}}
                child {node {s}}
                child {node {e}}
            }
            child {node {\textvisiblespace}}
            child {node {d}}
            child {node {o}}
            child {node {\textvisiblespace}}
            child {node {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {n}}
                }
            };

        \end{tikzpicture}
    };

    % right tree (placed in same row)
    \node[right=of lefttree] (righttree) {
        \begin{tikzpicture}[
            level distance=12mm,
            level 1/.style={sibling distance=5mm}, % children of the root
            level 2/.style={sibling distance=4mm}, % grandchi1
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {n}}
                }
            }
            child {node {\textvisiblespace}}
            child {node {;}}
            child {node {\textvisiblespace}}
            child {node {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {(}}
                    child {node {\textvisiblespace}}
                    child {node {$t_3$}
                        child {node {n}}
                    }
                    child {node {\textvisiblespace}}
                    child {node {+}}
                    child {node {\textvisiblespace}}
                    child {node {$t_3$}
                        child {node {n}}
                    }
                    child {node {\textvisiblespace}}
                    child {node {)}}
                }
            };
        \end{tikzpicture}
    };
\end{tikzpicture}

(4) Bubble $t_8 \rightarrow t_7 \text{\textvisiblespace} \& \text{\textvisiblespace} t_7$ ; merge ($t_8, t_7$) into $t_7$

$\big\downarrow$


\begin{tikzpicture}[node distance=10mm, every node/.style={align=center}]
    % left tree
    \node (lefttree) {
        \begin{tikzpicture}[
            level distance=10mm,
            sibling distance=4mm,
            level 1/.style={sibling distance=6mm}, % children of the root
            level 2/.style={sibling distance=5mm}, % grandchi1
            level 3/.style={sibling distance=4mm}, % grandchi1
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {w}}
            child {node {h}}
            child {node {i}}
            child {node {l}}
            child {node {e}}
            child {node {\textvisiblespace}}
            child {node [fill=orange]{$t_7$}
                child {node {$t_7$}
                    child {node {t}}
                    child {node {r}}
                    child {node {u}}
                    child {node {e}}
                }
                child {node {\textvisiblespace}}
                child {node {\&}}
                child {node {\textvisiblespace}}
                child {node {$t_7$}
                    child {node {f}}
                    child {node {a}}
                    child {node {l}}
                    child {node {s}}
                    child {node {e}}
                }
            }
            child {node {\textvisiblespace}}
            child {node {d}}
            child {node {o}}
            child {node {\textvisiblespace}}
            child {node {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {n}}
                }
            };

        \end{tikzpicture}
    };

    % right tree (placed in same row)
    \node[right=of lefttree] (righttree) {
        \begin{tikzpicture}[
            level distance=12mm,
            level 1/.style={sibling distance=5mm}, % children of the root
            level 2/.style={sibling distance=4mm}, % grandchi1
            every node/.style={draw=none,inner sep=1pt,font=\small, anchor=base},
            edge from parent/.style={draw}
        ]
        \node {$t_0$}
            child {node {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {n}}
                }
            }
            child {node {\textvisiblespace}}
            child {node {;}}
            child {node {\textvisiblespace}}
            child {node {$t_0$}
                child {node {L}}
                child {node {\textvisiblespace}}
                child {node {=}}
                child {node {\textvisiblespace}}
                child {node {$t_3$}
                    child {node {(}}
                    child {node {\textvisiblespace}}
                    child {node {$t_3$}
                        child {node {n}}
                    }
                    child {node {\textvisiblespace}}
                    child {node {+}}
                    child {node {\textvisiblespace}}
                    child {node {$t_3$}
                        child {node {n}}
                    }
                    child {node {\textvisiblespace}}
                    child {node {)}}
                }
            };
        \end{tikzpicture}
    };
\end{tikzpicture}
\caption{Example full run of ARVADA bubbling using non-tokenisation nor optimised tree, and the grammar induced (part 2)\cite{kulkarniLearningHighlyRecursive2021}}
\label{fig:fullrun2}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Why replicate ARVADA?}
The replication study of GLADE \cite{bastaniSynthesizingProgramInput} conducted by researchers at CISPA \cite{bendrissouSynthesizingInputGrammars2022} in 2022 reported results that were inconsistent with those presented in the original paper \cite{bastaniSynthesizingProgramInput}. Since ARVADA is closely related to GLADE, sharing a similar purpose and experimental setting, there is reason to suspect that ARVADA may also yield inconsistent results upon reimplementation.

\vspace{\baselineskip}

Another motivation for replication arises from the incomplete nature of the ARVADA study and its evaluation. Although the paper presents a reasonable amount of testing and statistical data, compares the algorithm with GLADE, and provides a detailed explanation of ARVADA, it does not offer formal guarantees. It remains unclear whether ARVADA is applicable in all scenarios or what subset of scenarios it can reliably handle. Furthermore, when comparing ARVADA with GLADE, the possibility exists that the grammars used for testing were selected based on performance considerations. In later research, it has been stated that ARVADA only has high F1 scores when input strings are relatively small, and can vary widely on each run \cite{arefinFastDeterministicBlackbox2024}, raising suspicions that the study may have been selective.

\subsection{Why C?}

In the original study \cite{kulkarniLearningHighlyRecursive2021}, the ARVADA algorithm was implemented in Python. When compared to GLADE \cite{bastaniSynthesizingProgramInput}, which was implemented in Java, ARVADA exhibited a slower average runtime across all benchmarks. In the study, this was attributed to the natural runtime disadvantage of Python compared to Java, which is valid; however, the possibility that ARVADA itself might be inherently slow was not acknowledged.

\vspace{\baselineskip}

In a comparative study, A Pragmatic Comparison of Four Different Programming Languages \cite{aliPragmaticComparisonFour2021}, it was found that when speed and efficiency are prioritised, C is a better choice than Python. As a mid-level, statically typed, and structured language that runs under a compiler, C consistently outperforms dynamically typed, interpreted languages such as Python \cite{kumarPythonLanguageComparison2022}. Moreover, C’s provision of only essential features contributes to its efficiency but also increases programming complexity compared to Python \cite{aliPragmaticComparisonFour2021, kumarPythonLanguageComparison2022}.

\vspace{\baselineskip}

Therefore, to investigate and potentially address runtime bottlenecks, C was selected as the implementation language for this replication study. The added complexity inherent to programming in C, relative to Python, was also acknowledged.

\subsection{Why is learning input grammar important?}
Grammar inference is important for many software engineering tasks, as knowledge about a program's grammar helps with code comprehension, reverse engineering, detecting and refactoring code smells, transforming source code for optimisation or bug fixing, and generating test inputs \ cite {arefinFastDeterministicBlackbox2024}. However, due to the increasing restrictions in many software systems—caused by global privacy and security regulations—we often lack access to the source code needed to learn their grammar.  Even with open-source programs and code, many only have closed-source parsers, making white-box or grey-box instrumentation difficult \cite{arefinFastDeterministicBlackbox2024,liIncrementalContextfreeGrammar2024}.

\vspace{\baselineskip}
This limitation highlights the importance of learning input grammars, which are formal grammars defining valid program inputs, and approaches that treat a program as a black box and recursively apply input grammars to infer or reverse-engineer the underlying grammar of the program.


\subsection{Related Work}

\subsection{TREEVADA}

Published in \enquote{Fast Deterministic Black-box Context-free Grammar inference} 2023, at the University of Texas\cite{arefinFastDeterministicBlackbox2024}, TREEAVADA is an algorithm that is based on ARVADA\cite{kulkarniLearningHighlyRecursive2021}, aiming to solve the non-deterministic and speed-related limitations of ARVADA. The paper claims, TREEVADA yields better quality grammar in a single run, with faster run time.

\vspace{\baselineskip}
To achieve such results, TREEVADA uses a few different techniques. First, during pre-tokenisation, TREEVADA pre-structures its parse according to nesting rules induced by balanced brackets common in many grammars, uses \textcolor{blue}{' "} quotes grouping and identifying string literals, and the same heuristics used in ARVADA, such as lowercase and numbers. Essentially, adding more structure to the initial parse trees compared to ARVADA, which does not consider brackets. Noting that TREEVADA assumes that the program uses \textcolor{blue}{' "} quotes to wrap a string and brackets for nesting only.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
\node {$t_0$}
        child {node {while}}
        child {node {\textvisiblespace}}
        child {node {n}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {u}}
        child {node {(}}
        child {node {\textvisiblespace}}
        child {node {n}}
        child {node {\textvisiblespace}}
        child {node {+}}
        child {node {n}}
        child {node {\textvisiblespace}}
        child {node {)}}
        child {node {\textvisiblespace}}
        child {node {do}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}
\caption{Naive Parse trees after pre-tokenisation in ARVADA}

\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
\node {$t_0$}
        child {node {while}}
        child {node {\textvisiblespace}}
        child {node {n}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {u}}
        child {node[fill=green] {$t_1$}
            child {node {(}}
            child {node {\textvisiblespace}}
            child {node {n}}
            child {node {\textvisiblespace}}
            child {node {+}}
            child {node {\textvisiblespace}}
            child {node {n}}
            child {node {\textvisiblespace}}
            child {node {)}}
        }
        child {node {\textvisiblespace}}
        child {node {do}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}
\caption{Naive Parse trees after pre-tokenisation in TREEVADA \cite{arefinFastDeterministicBlackbox2024}}
\label{fig:pre-tokenised trees}
\end{figure}

Secondly, to address the non-deterministic aspect of ARVADA, TREEAVADA switches to a deterministic data structure for specific operations and discards all bubbles with unmatched parentheses when bubbling. Additionally, two new heuristics, bubble length and bubble depth, are considered when considering bubble ranks during bubbling.

\subsection{KEDAVRA}

Proposed in \enquote{Incremental Context-free Grammar Inference in Black Box
Settings} \cite{liIncrementalContextfreeGrammar2024} in 2024, KEDAVRA is an algorithm designed to improve upon TREEVADA, in the same setting. Although TREEAVADA did improve upon ARVADA, it still had limitations of low accuracy, slow processing speeds, and limited readability due to complex grammar structures, which were inherited from ARVADA \cite{liIncrementalContextfreeGrammar2024,kulkarniLearningHighlyRecursive2021}. 
The paper highlights that KEDAVRA outperforms ARVADA and TREEVADA in terms of grammar precision, recall, runtime/computational efficiency, and readability while maintaining similar memory usage.

\vspace{\baselineskip}
The approach taken by KEDAVRA consists of 3 main parts:
\begin{itemize}
    \item Tokenisation
    \item Data Decomposition
    \item Incremental grammar inference
\end{itemize}

\subsubsection{Tokenisation}
Similar to ARVADA and TREEVADA, KEDAVRA performs a tokenisation step. However, unlike ARVADA, which is tokenised based on class, and TREEAVADA, which extends it to consider brackets. KEDAVRA tokenises based on common lexical rules such as identifiers, strings, and numbers, and only considers whitespace if the given $\mathcal{O}$ is sensitive to it. 

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=7mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
\node {$t_0$}
        child {node {if}}
        child {node {(}}
        child {node {a}}
        child {node {)}}
        child {node {;}}
        child {node {else}}
        child {node {b}}
        child {node {=}}
        child {node {5}}
        child {node {;}};
\end{tikzpicture}
\caption{Result during KEDAVRA pre-tokenisation given input \texttt{if(a); else b = 5;}}
\label{fig:pre-tokenising KEDAVRA}
\end{figure}

Furthermore, KEDAVRA during grammar inference performs \textbf{character-level generalisation} for token values. Given a token character type,  it computes all possible characters corresponding to that character type. For instance, if a token is a lowercase letter, all lowercase letters are included in the set.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=7mm,
    every node/.style={draw=none, inner sep=2pt, font=\small, anchor=base},
    edge from parent/.style={draw}
]
\node {$t_0$}
        child {node {if}}
        child {node {(}}
        child {node {$t_1$}}
        child {node {)}}
        child {node {;}}
        child {node {else}}
        child {node {$t_1$}}
        child {node {=}}
        child {node {$t_2$}}
        child {node {;}};
\end{tikzpicture}

\[
t_1 \rightarrow \text{\enquote{a}}|\text{\enquote{b}}|...|\text{\enquote{z}}
\]

\caption{Result after KEDAVRA pre-tokenisation given input \texttt{if(a); else b = 5;}, and example generalisation of $t_1$}
\label{fig:pre-tokenised KEDAVRA}
\end{figure}

\subsubsection{Data Decompostion}

KEDAVRA then breaks down all complex example strings into simple components, while collectively preserving all grammatical structure of the original sequence and essentially, breaking the input string into smaller valid input strings. This is done to overcome the slowdown and lack of readability of generated grammars due to the complexity inherent in the entire example strings.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    every node/.style={font=\ttfamily},
]

\node (l1) at (0,0) {;};
\node[below=0.6em of l1] (l2) {t2 ;};
\node[below=0.6em of l2] (l3) {t1 ;};
\node[below=0.6em of l3] (l4) {t1 = t2 ;};
\node[below=0.6em of l4] (l5) {if ( t1 ) ;};
\node[below=0.6em of l5] (l6) {while ( t2 ) ;};
\node[below=0.6em of l6] (l7) {if ( t1 ) ; else ;};

\end{tikzpicture}
\caption{Decomposed Sequences of toeknised example from \ref{fig:pre-tokenised KEDAVRA}}
\label{fig:decomp KEDAVRA}
\end{figure}

\subsubsection{Incremental Grammar inference}

After Decomposition, KEDAVRA orders all the decomposed sequences by length and begins to infer grammar from the simplest token sequence, iterating over all token sequences derived from the input strings. Done similarly to ARVADA and TREEAVADA, KEDAVRA uses bubbling and merging. Also, like TREEVADA, only consider bubbles that have matched parentheses if parentheses are included. 

\subsection{NATGI}

Published in \enquote{Black-Box Context-free Grammar Inference for Readable \& Natural Grammars} at the University of Texas 2025\cite{arefinBlackboxContextfreeGrammar2025}, NATGI is a novel LLM-guided grammar inference framework that extends TREEAVADA's parse tree recovery. Aiming to improve upon the difficulties of human readability and the lack of guarantees on larger and realistic languages of previous work.

\vspace{\baselineskip}

NATGI, as mentioned above, builds upon TREEAVADA's idea of bracket-induced tree structuring. It follows the same pre-tokenising step, adding two extra modifications. First, there is no separation of class between lowercase and uppercase letters, treating them as a single class. Second, it attempts to remove redundant whitespaces by iteratively removing whitespaces and checking if the resulting program is still valid. During this process, and each time a non-terminal is created via a merge, NATGI collects the subtrees that are about to be relabelled and prompts LLM models, given the sub-trees about to be merged for a more descriptive label compared to $t_1, t_2 ...$ .

\vspace{\baselineskip}
Then, before using any bubble heuristics process like ARVADA, TREEAVADA or KEDAVRA, NATGI further exploits the brackets. It automatically treats any sequence within enclosed brackets as a bubble and attempts to further structuralise the parse trees. This partially structured tree is then given to an LLM model with a specialised prompt to finish. If the LLM terminates due to LLM's limitations, such as hallucinations \cite{orvalhoAreLargeLanguage2025a,arefinBlackboxContextfreeGrammar2025}, it then uses TREEAVADA heuristics and bubbling to compensate.

\vspace{\baselineskip}
To allow a broader range of rules compared to the ones induced solely from the parse tree, NATGI decomposes each parse tree into smaller fragments. Allowing the resulting grammar to capture a broader set of language rules. This is done via effective pruning through the use of Hierarchical Delta Debugging (HDD). Pruning is the selective deletion of branches of a tree. HDD is an algorithm that iteratively prunes the trees as long as it fails a certain criterion \cite{misherghiHDDHierarchicalDelta2006a}. 

\vspace{\baselineskip}
Finally, NATGI performs lexical expansion. Since the leaf nodes are limited by the token in the input string, NATGI takes these leaf nodes and tries to expand the lexicon accepted. For example, if leaf token is a single character, NATGI attempts to see if multiple characters are accepted.


\subsection{Problem Statement}

Due to the unknowns and gaps in the original paper \cite{kulkarniLearningHighlyRecursive2021}—including the lack of guarantees regarding the kinds of grammars ARVADA can handle and the complexity of the grammars it can learn—along with the suspicion that the original study may have been selective in nature, this thesis aims to achieve the following:

\vspace{\baselineskip}
A replication of the study “Learning Highly Recursive Input Grammars” \cite{kulkarniLearningHighlyRecursive2021}, through a clean-room re-implementation of ARVADA, with the intention of expanding its evaluation to include a wider variety of grammars and identifying its key characteristics.

\vspace{\baselineskip}
The clean-room re-implementation will mean development is entirely from scratch in a new environment, based solely on the explanations provided in the original paper. This will also enable an assessment of the adequacy of those explanations—specifically, whether a complete re-implementation is possible based on the information given or if the paper lacks essential details for reproducability.

\vspace{\baselineskip}
Although the primary goal of this study is to re-implement, assess, and expand upon the original work, I acknowledge that this study may be incomplete or no new knowledge may be discovered. While this may seem trivial—merely confirming the findings of the previous study without adding novel insights \cite{hendriksConsiderItParsed}, replication studies play a vital role in reinforcing the trustworthiness and confidence of empirical results, which is a central tenet of the scientific method. Replication can increase certainty when findings are reproduced and promote innovation when they are not \cite{shepperdReplicationStudiesConsidered2018}. Therefore, this study holds significance within the field of computer science.







