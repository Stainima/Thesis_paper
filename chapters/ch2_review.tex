\chapter{GENERAL REVIEW OF LITERATURE}

\subsection{What are Grammars?}

Grammars in computer science can be defined as a set of rules by which valid sentences in a language are constructed \cite{jiangFormalGrammarsLanguages}, serving as a blueprint for the language. Beginning with a start symbol, which is a single non-terminal, production rules are applied sequentially, adding symbols from the alphabet according to the grammar’s production rules, to derive a string that is valid in the language \cite{GrammarTheoryComputation2025}.

\vspace{\baselineskip}

A grammar can be represented by the tuple $<N, T, P, S>$, where:
\begin{itemize}
\item $N$ is a finite, non-empty set of non-terminal symbols/alphabet.
\item $T$ is a finite set of terminal symbols/alphabet.
\item $P$ is a finite set of production rules.
\item $S$ is the start symbol (a non-terminal symbol/alphabet).
\end{itemize}

Here, the production rules $P$ define all symbol substitutions that can be performed recursively to generate different symbol sequences, known as \emph{strings} \cite{GrammarTheoryComputation2025}. These rules are written in the form $A \rightarrow w$, where $A \in N$ and $w \in (N \cup T)^*$.

\vspace{\baselineskip}
These sets of rules are used by parsers to parse a string of tokens and analyse its syntax against this set of rules(Grammars)\cite{mulikComparisonParsingTechniques,hendriksConsiderItParsed}.

\subsection{What are Context-Free Grammars?}

Depending on the set of production rules, each grammar can be classified according to the \emph{Chomsky hierarchy} \cite{chomskyTHREEMODELSTIE1956}.

\begin{center}
    \includegraphics[width=0.7\textwidth]{chapter1/ch_heir.png}
    \captionof{figure}{Chomsky hierarchy \cite{ChomskyHierarchyTheory2025}}
    \label{fig:hierarchy}
\end{center}

The hierarchy outlines four distinct types of grammars, ranging from \textbf{Type-3} (the most restrictive) to \textbf{Type-0} (the most general and unstructured). \textbf{Type-2} grammars in this hierarchy are known as \emph{context-free grammars}.

\subsubsection{Type-3}
Type-3 grammars, known as regular grammars, are used to generate regular languages. A grammar is classified as a regular grammar if its production rules follow from $X \rightarrow a | X \rightarrow aY$. The left-hand side must consist of a single $T$ and the right-hand side must consist of a single $T$ or a single $T$ and a single $N$.

Regular grammar can take 2 forms, right linear and left linear.

Right-linear takes the following form, where the $N$ on the right-hand side of the arrow is on the far right.
\begin{itemize}
    \item[] $X \rightarrow a$
    \item[] $X \rightarrow aB$
    \item[] $X \rightarrow \varepsilon$
\end{itemize}

Left-linear take the following form, where the $N$ on the right-hand side of the arrow is on the far left.
\begin{itemize}
    \item[] $X \rightarrow a$
    \item[] $X \rightarrow Ba$
    \item[] $X \rightarrow \varepsilon$
\end{itemize}

Note that right-linear and left-linear forms cannot be mixed, 
Doing so may generate languages that are not regular. 
Additionally, production of $\varepsilon$ is allowed only if the corresponding nonterminal 
does not appear on the right-hand side of any production rule \cite{hendriksConsiderItParsed,shiIntelligenceScience2021}

\subsubsection{Type-2}

Type-2 grammars, commonly known as context-free grammars (CFGs), have production rules of the form:
\[
A \rightarrow \alpha
\]
where \( A \in N \) and \( \alpha \in (T \cup N)^* \), meaning any string composed of terminal and nonterminal symbols \cite{hendriksConsiderItParsed,shiIntelligenceScience2021}.

Due to the nature of the right-hand side, nonterminals are allowed to recursively expand, which can lead to repetition. For example:
\begin{itemize}
    \item[] \( A \rightarrow sAb \)
    \item[] \( A \rightarrow \varepsilon \)
\end{itemize}
can produce derivations of the form
\[
A \Rightarrow sAb \Rightarrow ssAbb \Rightarrow \cdots \Rightarrow s^n b^n,
\]
until eventually \( A \Rightarrow \varepsilon \).

This property is particularly useful because it allows the grammar to enforce well-formed parentheses expressions and other recursive structures purely through production rules. As a result, context-free grammars are of great importance in the design and specification of programming languages\cite{hendriksConsiderItParsed,shiIntelligenceScience2021}.

\subsubsection{Type-1}

Type-1 grammars, also known as context-sensitive grammars, have production rules of the form:
\[
\alpha A \beta \rightarrow \alpha \gamma \beta
\]
where \( A \in N \) and \( \alpha, \beta, \gamma \in (N \cup T)^* \). This means that \( A \) can be expanded to \( \gamma \) only in the specific context where it appears between \( \alpha \) and \( \beta \) in the given order \cite{hendriksConsiderItParsed,shiIntelligenceScience2021}.

\vspace{\baselineskip}
A key property of context-sensitive grammars is that production rules must be \emph{non-contracting}: the length of the right-hand side must be greater than or equal to the length of the left-hand side. This implies that no nonterminal on the right-hand side can derive \( \varepsilon \), meaning nullable productions are not allowed.

An example derivation using a context-sensitive production is:
\[
A \Rightarrow aA\beta \Rightarrow aaA\beta\beta \Rightarrow \cdots
\]

\subsubsection{Type-0}

Type-0 grammars, also known as unrestricted grammars, sit at the top of Chomsky's hierarchy. Their production has a complete lack of restrictions and takes the general form:
\[
\alpha \rightarrow \beta
\]
where \( \alpha, \beta \in (N \cup T)^+ \), meaning they are strings consisting of terminal and nonterminal symbols. The only requirement is that \( \alpha \) must contain at least one nonterminal symbol to allow for further derivations \cite{hendriksConsiderItParsed,shiIntelligenceScience2021}.

Type-0 grammars are the most expressive class in the Chomsky hierarchy and can generate all recursively enumerable languages.

\subsection{What is ARVADA?}

\subsubsection{Introduction}

ARVADA is an algorithm published in \enquote{Learning Highly Recursive Input Grammars} \cite{kulkarniLearningHighlyRecursive2021} at the University of California, Berkeley in 2021. It is designed to learn context-free grammars from a set of positive examples and a Boolean-valued oracle $\mathcal{O}$. Starting from initially flat parse trees, ARVADA repeatedly applies two specialised operations, \textbf{bubbling} and \textbf{merging}, to incrementally add structure to these trees. From this structured representation, it extracts the smallest possible set of context-free grammar rules that accommodate all the given examples. The algorithm aims to generalise the language as much as possible without overgeneralising beyond what is accepted by $\mathcal{O}$.

\vspace{\baselineskip}
Like GLADE \cite{bastaniSynthesizingProgramInput}, ARVADA operates under the assumption of a black-box oracle $\mathcal{O}$. This means that ARVADA has no access to or knowledge of the internal workings of the oracle and can only observe the Boolean values returned by $\mathcal{O}$.

\subsubsection{Explanation}

ARVADA takes as input the oracle $\mathcal{O}$ and a set of positive, valid oracle inputs $S$. For each string $s \in S$, querying $\mathcal{O}(s)$ returns \verb|True|. The algorithm begins by constructing a flat parse tree for each string in $S$. Each tree has a single root node $t_0$ whose children correspond to the individual characters of the input string $s$.

\vspace{\baselineskip}
Next, ARVADA performs the \textbf{bubbling} operation. In this step, a sequence of sibling nodes in the tree is selected and replaced with a new non-terminal node. This new node takes the selected sibling nodes as its children, thereby introducing an additional level of structure. Essentially, ARVADA transforms sequences of terminal nodes in the flat parse tree into subtrees by introducing new non-terminal nodes and progressively adding structure to the tree.

\vspace{\baselineskip}
ARVADA then decides whether to accept or reject each bubble by checking whether the newly bubbled structure enables a sound generalisation of the learned grammar. Each non-leaf node in the tree can be viewed as a non-terminal in the emerging grammar. To determine whether a bubble should be accepted, ARVADA checks whether replacing any node in the tree with the new bubbled subtree results in the generation of valid input strings according to $\mathcal{O}$. If the replacement produces valid strings, the bubble is accepted, and the tree is restructured so that both the bubbled subtree and the replaced node share the same non-terminal label.

\vspace{\baselineskip}
The addition of new non-terminal nodes expands the language defined by the learned grammar, since any string derivable from the same label can now be substituted interchangeably. This relabeling of the bubbled subtree and the replaced node is called a \textbf{merge}, as it merges the labels of two previously distinct nodes in the tree. If a bubble is not accepted, it is discarded, and none of the trees are affected or structurally modified.

\subsubsection{Walkthrough}

This walkthrough will follow very closely to the examples provided in the original paper \cite{kulkarniLearningHighlyRecursive2021}, and use a concrete example to provide an in-depth understanding of ARVADA.

%%%%%%%%% GRAMMAR %%%%%%%%%%%
\begin{figure}[H]
\begin{tcolorbox}[title=$G_w$, colback=white, colframe=black]
\begin{grammar}{
    \pr{\emph{start}}{\emph{stmt}}
    
    \pr{\emph{stmt}}{while\textvisiblespace\ \emph{boolexpr}\textvisiblespace\ do\textvisiblespace\ \emph{stmt}}
    & & \gors if\textvisiblespace\ \emph{boolexpr}\textvisiblespace\ then\textvisiblespace\ \emph{stmt}\textvisiblespace\ else\textvisiblespace\ \emph{stmt}\\
    & & \gors L\textvisiblespace\ =\textvisiblespace\ \emph{numexpr}\\
    & & \gors \emph{stmt}\textvisiblespace\ ;\textvisiblespace\ \emph{stmt}\\
    
    \pr{\emph{boolexpr}}{$\thicksim$\emph{boolexpr}}
    & & \gors \emph{boolexpr}\textvisiblespace\ \&\textvisiblespace\ \emph{boolexpr}\\
    & & \gors \emph{numexpr}\textvisiblespace\ ==\textvisiblespace\ \emph{numexpr}\\
    & & \gors false\\
    & & \gors true\\
    
    \pr{\emph{numexpr}}{(\textvisiblespace\ \emph{numexpr}\textvisiblespace\ +\textvisiblespace\ \emph{numexpr}\textvisiblespace\ )}
    & & \gors L\\
    & & \gors n\\
}
\end{grammar}
\end{tcolorbox}

\[
S = \left\{
\text{\texttt{while true \& false do L = n}},
\quad
\text{\texttt{L = n ; L = (n+n)}}
\right\}
\]

\[
O(i) =
\begin{cases}
\text{True} & \text{if } i \in \mathcal{L}(G_w) \\
\text{False} & \text{otherwise}
\end{cases}
\]
\caption{Definition a simple while grammar $G_w$, sample input strings $S$, and oracle $\mathcal{O}$ \cite{kulkarniLearningHighlyRecursive2021}}
\label{fig:grammar}
\end{figure}
%%%%%%%%%%%%%%%%%%%

%%% proofread here please %%%%%
For this walkthrough, the simple while grammar $G_w$ and input string $S$ provided in figure~\ref{fig: grammar} will be used. Noting again that ARVADA treats $\mathcal{O}$ as a black box, meaning it has no structural knowledge of $G_w$, and $G_w$ is only shown to clarify the behaviour of $\mathcal{O}$.

%%%%% Pseudocode $$$$$$
\begin{algorithm}[H]
\caption{High-level overview of ARVADA \cite{kulkarniLearningHighlyRecursive2021}}\label{alg:arv}
\begin{algorithmic}[1]
    \Require a set of examples $S$, a language oracle $\mathcal{O}$.
    \State $bestTrees \gets \textsc{NaiveParseTrees}(S)$
    \State $bestTrees \gets \textsc{MergeAllValid}(bestTrees, \mathcal{O})$
    \State $updated \gets \text{True}$
    \While{$updated$}
        \State $updated \gets \text{False}$
        \State $allBubbles \gets \textsc{GetBubbles}(bestTrees)$
        \For{$bubble$ \textbf{in} $allBubbles$}
            \State $bbldTrees \gets \textsc{Apply}(bestTrees, bubble)$
            \State $accepted, mergedTs \gets \textsc{CheckBubble}(bbldTrees, \mathcal{O})$
            \If{$accepted$}
                \State $bestTrees \gets mergedTs$
                \State $updated \gets \text{True}$
                \State \textbf{break}
            \EndIf
        \EndFor
    \EndWhile
    \State $G \gets \textsc{InducedGrammar}(bestTrees)$
    \State \textbf{Return} $G$
\end{algorithmic}
\end{algorithm}

ARVADA follows the high-level overview provided in algorithm ~\ref{alg:arv}, and begins by taking all the input strings in $S$ and building a naive flat parse tree for each input string.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small},
    edge from parent/.style={draw}
]
    \node {$t_0$}
        child {node {w}}
        child {node {h}}
        child {node {i}}
        child {node {l}}
        child {node {e}}
        child {node {\textvisiblespace}}
        child {node {t}}
        child {node {r}}
        child {node {u}}
        child {node {e}}
        child {node {\textvisiblespace}}
        child {node {\&}}
        child {node {\textvisiblespace}}
        child {node {f}}
        child {node {a}}
        child {node {l}}
        child {node {s}}
        child {node {e}}
        child {node {\textvisiblespace}}
        child {node {d}}
        child {node {o}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}

\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small},
    edge from parent/.style={draw}
]
\node {$t_0$}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {;}}
    child {node {\textvisiblespace}}
    child {node {L}}
    child {node {\textvisiblespace}}
    child {node {=}}
    child {node {\textvisiblespace}}
    child {node {(}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {+}}
    child {node {\textvisiblespace}}
    child {node {n}}
    child {node {\textvisiblespace}}
    child {node {)}};    
\end{tikzpicture}
\caption{Initial set of flat naive parse trees ARVADA builds given inputs $S$, where each $t_i$ is a non-terminal}
\label{fig:initialNaiveTrees}
\end{figure}

In this initially naive parse tree, for each string input. There is a non-terminal $t_0$, and all characters in the string, including the spaces, are children nodes.

\[
t_0 \to \text{w\;h\;i\;l\;e\;\textvisiblespace\;t\;r\;u\;e\textvisiblespace\&\;f\;a\;l\;s\;e\;\textvisiblespace\;d\;o\;\textvisiblespace\;L\;\textvisiblespace=\;\textvisiblespace\;n}
\]
\[
t_0 \to \text{L\;\textvisiblespace\;=\;\textvisiblespace \;n\;\textvisiblespace\;;\;\textvisiblespace\;L\;\textvisiblespace\;=\;\textvisiblespace\;(\;n\;+\;n\;)}
\]

\subsection{Why replicate ARVADA?}
The replication study of GLADE \cite{bastaniSynthesizingProgramInput} conducted by researchers at CISPA \cite{bendrissouSynthesizingInputGrammars2022} in 2022 reported results that were inconsistent with those presented in the original paper \cite{bastaniSynthesizingProgramInput}. Since ARVADA is closely related to GLADE, sharing a similar purpose and experimental setting, there is reason to suspect that ARVADA may also yield inconsistent results upon reimplementation.

\vspace{\baselineskip}

Another motivation for replication arises from the incomplete nature of the ARVADA study and its evaluation. Although the paper presents a reasonable amount of testing and statistical data, compares the algorithm with GLADE, and provides a detailed explanation of ARVADA, it does not offer formal guarantees. It remains unclear whether ARVADA is applicable in all scenarios or what subset of scenarios it can reliably handle. Furthermore, when comparing ARVADA with GLADE, there is a possibility that the grammars used for testing were selected based on performance considerations. In later research, it has been stated that ARVADA only has high F1 scores when input strings are relatively small, and can vary widely on each run \cite{arefinFastDeterministicBlackbox2024}, raising suspicions that the study may have been selective.

\subsection{Why C?}

In the original study \cite{kulkarniLearningHighlyRecursive2021}, the ARVADA algorithm was implemented in Python. When compared to GLADE \cite{bastaniSynthesizingProgramInput}, which was implemented in Java, ARVADA exhibited a slower average runtime across all benchmarks. In the study, this was attributed to the natural runtime disadvantage of Python compared to Java, which is valid; however, the possibility that ARVADA itself might be inherently slow was not acknowledged.

\vspace{\baselineskip}

In a comparative study, A Pragmatic Comparison of Four Different Programming Languages \cite{aliPragmaticComparisonFour2021}, it was found that when speed and efficiency are prioritized, C is a better choice than Python. As a mid-level, statically typed, and structured language that runs under a compiler, C consistently outperforms dynamically typed, interpreted languages such as Python \cite{kumarPythonLanguageComparison2022}. Moreover, C’s provision of only essential features contributes to its efficiency but also increases programming complexity compared to Python \cite{aliPragmaticComparisonFour2021, kumarPythonLanguageComparison2022}.

\vspace{\baselineskip}

Therefore, to investigate and potentially address runtime bottlenecks, C was selected as the implementation language for this replication study. The added complexity inherent to programming in C, relative to Python, was also acknowledged.


\subsection{Why is learning input grammar important?}
Grammar inference is important for many software engineering tasks, as knowledge about a program grammar helps with code comprehension, reverse engineering, detecting and refactoring code smells, transforming source code for optimization or bug fixing, and generating test inputs\cite{arefinFastDeterministicBlackbox2024}. However, due to the increasing restrictions in many software systems—caused by global privacy and security regulations—we often do not have access to the source code needed to learn their grammar.  Even upon open-source programs and code, many only have closed source prasers, making white-box or grey-box instrumention difficult \cite{arefinFastDeterministicBlackbox2024,liIncrementalContextfreeGrammar2024}.

\vspace{\baselineskip}
This limitation highlights the importance of learning input grammars, which are formal grammars defining valid program inputs, and approaches that treat a program as a black box and recursively apply input grammars to infer or reverse-engineer the underlying grammar of the program.


\subsection{Related Work}
\subsubsection{GLADE}
GLADE is an algorithm proposed by Bastani et al., published in Synthesizing Input Grammars at PLDI 2017 \cite{bastaniSynthesizingProgramInput}. Like ARVADA, GLADE uses a set of valid inputs and black-box access to the program, with the aim of automatically approximating the context-free input grammar of the given program.

\vspace{\baselineskip}

Because GLADE and ARVADA share a similar experimental setting, GLADE was used as a benchmark for ARVADA during its evaluation. It was shown that GLADE, on average, had a faster runtime compared to ARVADA. However, in terms of generalisation, following the original grammar of the oracle more closely, ARVADA outperformed GLADE across the 11 benchmarks, achieving a higher F1 score on 9 of the 11 benchmarks.

\subsection{TREEVADA}

Published in \enquote{Fast Deterministic Black-box Context-free Grammar inference} 2023, at the University of Texas\cite{arefinFastDeterministicBlackbox2024}, TREEAVADA is an algorithm that is based on ARVADA\cite{kulkarniLearningHighlyRecursive2021}, aiming to solve the non-deterministic and speed-related limitations of ARVADA. The paper claims, TREEVADA yields better quality grammar in a single run, with faster run time.

\vspace{\baselineskip}
To achieve such results, TREEVADA uses a few different techniques. First, during pre-tokenisation, TREEVADA pre-structures its parse according to nesting rules induced by balanced brackets common in many grammars, uses \textcolor{blue}{' "} quotes grouping and identifying string literals, and the same heuristics used in ARVADA, such as lowercase and numbers. Essentially, adding more structure to the initial parse trees compared to ARVADA, which does not consider brackets. Noting that TREEVADA assumes that the program uses \textcolor{blue}{' "} quotes to wrap a string and brackets for nesting only.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small},
    edge from parent/.style={draw}
]
\node {$t_0$}
        child {node {while}}
        child {node {\textvisiblespace}}
        child {node {n}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {u}}
        child {node {(}}
        child {node {\textvisiblespace}}
        child {node {n}}
        child {node {\textvisiblespace}}
        child {node {+}}
        child {node {n}}
        child {node {\textvisiblespace}}
        child {node {)}}
        child {node {\textvisiblespace}}
        child {node {do}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}
\caption{Naive Parse trees after pre-tokenisation in ARVADA}

\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=5mm,
    every node/.style={draw=none, inner sep=2pt, font=\small},
    edge from parent/.style={draw}
]
\node {$t_0$}
        child {node {while}}
        child {node {\textvisiblespace}}
        child {node {n}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {u}}
        child {node[fill=green] {$t_1$}
            child {node {(}}
            child {node {\textvisiblespace}}
            child {node {n}}
            child {node {\textvisiblespace}}
            child {node {+}}
            child {node {\textvisiblespace}}
            child {node {n}}
            child {node {\textvisiblespace}}
            child {node {)}}
        }
        child {node {\textvisiblespace}}
        child {node {do}}
        child {node {\textvisiblespace}}
        child {node {L}}
        child {node {\textvisiblespace}}
        child {node {=}}
        child {node {\textvisiblespace}}
        child {node {n}};
\end{tikzpicture}
\caption{Naive Parse trees after pre-tokenisation in TREEVADA \cite{arefinFastDeterministicBlackbox2024}}
\label{fig:pre-tokenised trees}
\end{figure}

Secondly, to address the non-deterministic aspect of ARVADA, TREEAVADA switches to a deterministic data structure for specific operations and discard all bubbles with unmatched parentheses when bubbling. Additionally, two new heuristics, bubble length and bubble depth, are considered when considering bubble ranks during bubbling.

\subsection{KEDAVRA}

Proposed in \enquote{Incremental Context-free Grammar Inference in Black Box
Settings} \cite{liIncrementalContextfreeGrammar2024} in 2024, KEDAVRA is an algorithm designed to improve upon TREEVADA, in the same setting. Although TREEAVADA did improve upon ARVADA, it still had limitations of low accuracy, slow processing speeds, and limited readability due to complex grammar structures, which were inherited from ARVADA \cite{liIncrementalContextfreeGrammar2024,kulkarniLearningHighlyRecursive2021}. 
The paper highlights that KEDAVRA outperforms ARVADA and TREEVADA in terms of grammar precision, recall, runtime/computational efficiency, and readability while maintaining similar memory usage.

\vspace{\baselineskip}
The approach taken by KEDAVRA consists of 3 main parts:
\begin{itemize}
    \item Tokenisation
    \item Data Decomposition
    \item Incremental grammar inference
\end{itemize}

\subsubsection{Tokenisation}
Similar to ARVADA and TREEVADA, KEDAVRA performs a tokenisation step. However, unlike ARVADA, which is tokenised based on class, and TREEAVADA, which extends it to consider brackets. KEDAVRA tokenises based on common lexical rules such as identifiers, strings, and numbers, and only considers whitespace if the given $\mathcal{O}$ is sensitive to it. 

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=7mm,
    every node/.style={draw=none, inner sep=2pt, font=\small},
    edge from parent/.style={draw}
]
\node {$t_0$}
        child {node {if}}
        child {node {(}}
        child {node {a}}
        child {node {)}}
        child {node {;}}
        child {node {else}}
        child {node {b}}
        child {node {=}}
        child {node {5}}
        child {node {;}};
\end{tikzpicture}
\caption{Result during KEDAVRA pre-tokenisation given input \texttt{if(a); else b = 5;}}
\label{fig:pre-tokenising KEDAVRA}
\end{figure}

Furthermore, KEDAVRA during grammar inference performs \textbf{character-level generalisation} for token values. Given a token character type,  it computes all possible characters corresponding to that character type. For instance, if a token is a lowercase letter, all lowercase letters are included in the set.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    level distance=15mm,
    sibling distance=7mm,
    every node/.style={draw=none, inner sep=2pt, font=\small},
    edge from parent/.style={draw}
]
\node {$t_0$}
        child {node {if}}
        child {node {(}}
        child {node {$t_1$}}
        child {node {)}}
        child {node {;}}
        child {node {else}}
        child {node {$t_1$}}
        child {node {=}}
        child {node {$t_2$}}
        child {node {;}};
\end{tikzpicture}

\[
t_1 \rightarrow \text{\enquote{a}}|\text{\enquote{b}}|...|\text{\enquote{z}}
\]

\caption{Result after KEDAVRA pre-tokenisation given input \texttt{if(a); else b = 5;}, and example generalisation of $t_1$}
\label{fig:pre-tokenised KEDAVRA}
\end{figure}

\subsubsection{Data Decompostion}

KEDAVRA then breaks down all complex example strings into simple components, while collectively preserving all grammatical structure of the original sequence and essentially, breaking the input string into smaller valid input strings. This is done to overcome the slowdown and lack of readability of generated grammars due to the complexity inherent in the entire example strings.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
    every node/.style={font=\ttfamily},
]

\node (l1) at (0,0) {;};
\node[below=0.6em of l1] (l2) {t2 ;};
\node[below=0.6em of l2] (l3) {t1 ;};
\node[below=0.6em of l3] (l4) {t1 = t2 ;};
\node[below=0.6em of l4] (l5) {if ( t1 ) ;};
\node[below=0.6em of l5] (l6) {while ( t2 ) ;};
\node[below=0.6em of l6] (l7) {if ( t1 ) ; else ;};

\end{tikzpicture}
\caption{Decomposed Sequences of toeknised example from \ref{fig:pre-tokenised KEDAVRA}}
\label{fig:decomp KEDAVRA}
\end{figure}

\subsubsection{Incremental Grammar inference}

After Decomposition, KEDAVRA orders all the decomposed sequences by length and begins to infer grammar from the simplest token sequence, iterating over all token sequences derived from the input strings. Done similarly to ARVADA and TREEAVADA, KEDAVRA uses bubbling and merging. Also, like TREEVADA, only consider bubbles that have matched parentheses if parentheses are included.  

\subsection{NATGI}

Published in \enquote{Black-Box Context-free Grammar Inference for Readbile \& Natural Grammars} at the University of Texas 2025, NATGI is a novel LLM-guided grammar inference framework that extends TREEAVADA's parse tree recovery.

\subsection{Problem Statement}

Due to the unknowns and gaps in the original paper \cite{kulkarniLearningHighlyRecursive2021}—including the lack of guarantees regarding the kinds of grammars ARVADA can handle and the complexity of the grammars it can learn—along with the suspicion that the original study may have been selective in nature, this thesis aims to achieve the following:

\vspace{\baselineskip}
A replication of the study “Learning Highly Recursive Input Grammars” \cite{kulkarniLearningHighlyRecursive2021}, through a clean-room re-implementation of ARVADA, with the intention of expanding its evaluation to include a wider variety of grammars and identifying its key characteristics.

\vspace{\baselineskip}
The clean-room re-implementation will mean development is entirely from scratch in a new environment, based solely on the explanations provided in the original paper. This will also enable an assessment of the adequacy of those explanations—specifically, whether a complete re-implementation is possible based on the information given or if the paper lacks essential details for reproducability.

\vspace{\baselineskip}
Although the primary goal of this study is to re-implement, assess, and explain upon the original work, I acknowledge that this study may be incomplete or no new knowledge may be discovered. While this may seem trivial—merely confirming the findings of the previous study without adding novel insights \cite{hendriksConsiderItParsed}, replication studies play a vital role in reinforcing the trustworthiness and confidence of empirical results, which is a central tenet of the scientific method. Replication can increase certainty when findings are reproduced and promote innovation when they are not \cite{shepperdReplicationStudiesConsidered2018}. Therefore, this study holds significance within the field of computer science.

