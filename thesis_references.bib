@online{aliPragmaticComparisonFour2021,
  title = {A {{Pragmatic Comparison}} of {{Four Different Programming Languages}}},
  author = {Ali, Saqib and Qayyum, Sammar},
  date = {2021-06-21},
  doi = {10.14293/S2199-1006.1.SOR-.PP5RV1O.v1},
  url = {https://scienceopen.com/hosted-document?doi=10.14293/S2199-1006.1.SOR-.PP5RV1O.v1},
  urldate = {2025-09-24},
  abstract = {Programming language debates are extremely common among programmers, computer science students and of course, software engineers. However, all of the abovementioned professionals can concur that different languages excel in different scenarios. Software Engineers and programmers working on different projects can easily use different languages for different tasks during their work. Every year different programming languages are designed and created. In this Research, we will keep in focus the Four Horsemen of programming languages: C, C++, Python and Java; with respect to the criteria of time, speed and simplicity. The same optimized piece of pseudo-code is used to write the code of the different programming languages mentioned above by following their respective syntax and rules. The results of the comparison will be displayed with the help of a table in order to simplify the final results for the reader.},
  langid = {english},
  pubstate = {prepublished}
}

@online{arefinBlackboxContextfreeGrammar2025,
  title = {Black-Box {{Context-free Grammar Inference}} for {{Readable}} \& {{Natural Grammars}}},
  author = {Arefin, Mohammad Rifat and Rahman, Shanto and Csallner, Christoph},
  date = {2025-09-30},
  eprint = {2509.26616},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.26616},
  url = {http://arxiv.org/abs/2509.26616},
  urldate = {2025-10-16},
  abstract = {Black-box context-free grammar inference is crucial for program analysis, reverse engineering, and security, yet existing tools such as Arvada, TreeVada, and Kedavra struggle with scalability, readability, and accuracy on large, complex languages. We present NatGI, a novel LLM-guided grammar inference framework that extends TreeVada's parse tree recovery with three key innovations: bracket-guided bubble exploration, LLM-driven bubble generation and non-terminal labeling, and hierarchical delta debugging (HDD) for systematic tree simplification. Bracket-guided exploration leverages syntactic cues such as parentheses to propose well-structured grammar fragments, while LLM guidance produces meaningful non-terminal names and selects more promising merges. Finally, HDD incrementally reduces unnecessary rules, which makes the grammars both compact and interpretable. In our experiments, we evaluate NatGI on a comprehensive benchmark suite ranging from small languages to larger ones such as lua, c, and mysql. Our results show that NatGI consistently outperforms strong baselines in terms of F1 score. On average, NatGI achieves an F1 score of 0.57, which is 25pp (percentage points) higher than the best-performing baseline, TreeVada. In the case of interpretability, our generated grammars perform significantly better than those produced by existing approaches. Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules with meaningful non-terminal names and compact structures that align more closely with human intuition. As a result, developers and researchers can achieve higher accuracy while still being able to easily inspect, verify, and reason about the structure and semantics of the induced grammars.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Formal Languages and Automata Theory,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/Users/sulavmalla/Zotero/storage/VAB2Z5RB/Arefin et al. - 2025 - Black-box Context-free Grammar Inference for Readable & Natural Grammars.pdf}
}

@online{arefinFastDeterministicBlackbox2024,
  title = {Fast {{Deterministic Black-box Context-free Grammar Inference}}},
  author = {Arefin, Mohammad Rifat and Shetiya, Suraj and Wang, Zili and Csallner, Christoph},
  date = {2024-01-17},
  eprint = {2308.06163},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.06163},
  url = {http://arxiv.org/abs/2308.06163},
  urldate = {2025-10-16},
  abstract = {Black-box context-free grammar inference is a hard problem as in many practical settings it only has access to a limited number of example programs. The state-of-the-art approach Arvada heuristically generalizes grammar rules starting from flat parse trees and is non-deterministic to explore different generalization sequences. We observe that many of Arvada’s generalization steps violate common language concept nesting rules. We thus propose to prestructure input programs along these nesting rules, apply learnt rules recursively, and make black-box context-free grammar inference deterministic. The resulting TreeVada yielded faster runtime and higher-quality grammars in an empirical comparison.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/Users/sulavmalla/Zotero/storage/JUDHCB49/Arefin et al. - 2024 - Fast Deterministic Black-box Context-free Grammar Inference.pdf}
}

@article{bastaniSynthesizingProgramInput,
  title = {Synthesizing {{Program Input Grammars}}},
  author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy},
  abstract = {We present an algorithm for synthesizing a context-free grammar encoding the language of valid program inputs from a set of input examples and blackbox access to the program. Our algorithm addresses shortcomings of existing grammar inference algorithms, which both severely overgeneralize and are prohibitively slow. Our implementation, GLADE, leverages the grammar synthesized by our algorithm to fuzz test programs with structured inputs. We show that GLADE substantially increases the incremental coverage on valid inputs compared to two baseline fuzzers.},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/2RG4RHY7/Bastani et al. - Synthesizing Program Input Grammars.pdf}
}

@inproceedings{bendrissouSynthesizingInputGrammars2022,
  title = {“{{Synthesizing}} Input Grammars”: A Replication Study},
  shorttitle = {“{{Synthesizing}} Input Grammars”},
  booktitle = {Proceedings of the 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Bendrissou, Bachir and Gopinath, Rahul and Zeller, Andreas},
  date = {2022-06-09},
  pages = {260--268},
  publisher = {ACM},
  location = {San Diego CA USA},
  doi = {10.1145/3519939.3523716},
  url = {https://dl.acm.org/doi/10.1145/3519939.3523716},
  urldate = {2025-09-08},
  abstract = {When producing test inputs for a program, test generators (łfuzzersž) can greatly profit from grammars that formally describe the language of expected inputs. In recent years, researchers thus have studied means to recover input grammars from programs and their executions. The GLADE algorithm by Bastani et al., published at PLDI 2017, was the first blackbox approach to claim context-free approximation of input specification for non-trivial languages such as XML, Lisp, URLs, and more. Prompted by recent observations that the GLADE algorithm may show lower performance than reported in the original paper, we have reimplemented the GLADE algorithm from scratch. Our evaluation confirms that the effectiveness score (F1) reported in the GLADE paper is overly optimistic, and in some cases, based on the wrong language. Furthermore, GLADE fares poorly in several real-world languages evaluated, producing grammars that spend megabytes to enumerate inputs.},
  eventtitle = {{{PLDI}} '22: 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  isbn = {978-1-4503-9265-5},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/8RXKN9GY/Bendrissou et al. - 2022 - “Synthesizing input grammars” a replication study.pdf}
}

@online{ChomskyHierarchyTheory2025,
  title = {Chomsky {{Hierarchy}} in {{Theory}} of {{Computation}}},
  date = {2025-07-23},
  url = {https://www.geeksforgeeks.org/theory-of-computation/chomsky-hierarchy-in-theory-of-computation/},
  urldate = {2025-10-08}
}

@thesis{chomskyTHREEMODELSTIE1956,
  type = {Papaer},
  title = {{{THREE MODELS FOR TIE DESCRIPTION OF LANGUAGE}}},
  author = {Chomsky, Noam},
  date = {1956},
  institution = {MIT},
  location = {Massachusetta},
  file = {/Users/sulavmalla/Zotero/storage/M2EC6BZ9/Chomsky - 1956 - THREE MODELS FOR TIE DESCRIPTION OF LANGUAGE.pdf}
}

@online{gopinathBuildingFastFuzzers2019,
  title = {Building {{Fast Fuzzers}}},
  author = {Gopinath, Rahul and Zeller, Andreas},
  date = {2019-11-18},
  eprint = {1911.07707},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.07707},
  url = {http://arxiv.org/abs/1911.07707},
  urldate = {2025-09-15},
  abstract = {Fuzzing is one of the key techniques for evaluating the robustness of programs against attacks. Fuzzing has to be effective in producing inputs that cover functionality and find vulnerabilities. But it also has to be efficient in producing such inputs quickly. Random fuzzers are very efficient, as they can quickly generate random inputs; but they are not very effective, as the large majority of inputs generated is syntactically invalid. Grammar-based fuzzers make use of a grammar (or another model for the input language) to produce syntactically correct inputs, and thus can quickly cover input space and associated functionality. Existing grammar-based fuzzers are surprisingly inefficient, though: Even the fastest grammar fuzzer dharma still produces inputs about a thousand times slower than the fastest random fuzzer. So far, one can have an effective or an efficient fuzzer, but not both. In this paper, we describe how to build fast grammar fuzzers from the ground up, treating the problem of fuzzing from a programming language implementation perspective. Starting with a Python textbook approach, we adopt and adapt optimization techniques from functional programming and virtual machine implementation techniques together with other novel domain-specific optimizations in a step-by-step fashion. In our F1 prototype fuzzer, these improve production speed by a factor of 100–300 over the fastest grammar fuzzer dharma. As F1 is even 5–8 times faster than a lexical random fuzzer, we can find bugs faster and test with much larger valid inputs than previously possible.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/Users/sulavmalla/Zotero/storage/85VCLLFY/Gopinath and Zeller - 2019 - Building Fast Fuzzers.pdf}
}

@online{GrammarTheoryComputation2025,
  type = {Blog},
  title = {Grammar in {{Theory}} of {{Computation}}},
  date = {2025-08-25},
  url = {https://www.geeksforgeeks.org/theory-of-computation/introduction-to-grammar-in-theory-of-computation/},
  urldate = {2025-10-07},
  organization = {Grammar in Theory of Computation}
}

@thesis{hendriksConsiderItParsed,
  type = {Thesis},
  title = {Consider It {{Parsed}}!},
  author = {Hendriks, Max and Zaytsev, Vadim},
  institution = {University of Twente},
  location = {Enschede, Netherlands},
  abstract = {Relational parsing with context-free memoization, first presented in 2020, promises generalised contextfree parsing at speeds exceeding that of the current state-of-the-art parser generator: ANTLR4. Here, we present an independent attempt at bringing relational parsing to life based only on its original documentation, identify challenges when implementing it, and examine the capabilities of relational parsing. Our implementation of relational parsing can parse some challenging types of grammars, half of which ANTLR4 could not. However, there is a type of grammar that we cannot parse and for most tested grammars we did not manage to improve parsing speed by using context-free memoization. We expect these two caveats result from the way we implemented relational parsing rather than the method itself. We discuss some open questions that need answering before relational parsing can take its place as a practical and usable context-free parser.},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/JSV2PSMD/Hendriks and Zaytsev - Consider it Parsed!.pdf}
}

@article{jiangFormalGrammarsLanguages,
  title = {Formal {{Grammars}} and {{Languages}}},
  author = {Jiang, Tao and Li, Ming and Ravikumar, Bala and Regan, Kenneth W},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/2YJ2YF76/Jiang et al. - Formal Grammars and Languages.pdf}
}

@inproceedings{kulkarniLearningHighlyRecursive2021,
  title = {Learning {{Highly Recursive Input Grammars}}},
  booktitle = {2021 36th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Kulkarni, Neil and Lemieux, Caroline and Sen, Koushik},
  date = {2021-11},
  pages = {456--467},
  publisher = {IEEE},
  location = {Melbourne, Australia},
  doi = {10.1109/ASE51524.2021.9678879},
  url = {https://ieeexplore.ieee.org/document/9678879/},
  urldate = {2025-09-08},
  abstract = {This paper presents ARVADA, an algorithm for learning context-free grammars from a set of positive examples and a Boolean-valued oracle. ARVADA learns a context-free grammar by building parse trees from the positive examples. Starting from initially flat trees, ARVADA builds structure to these trees with a key operation: it bubbles sequences of sibling nodes in the trees into a new node, adding a layer of indirection to the tree. Bubbling operations enable recursive generalization in the learned grammar. We evaluate ARVADA against GLADE and find it achieves on average increases of 4.98× in recall and 3.13× in F1 score, while incurring only a 1.27× slowdown and requiring only 0.87× as many calls to the oracle. ARVADA has a particularly marked improvement over GLADE on grammars with highly recursive structure, like those of programming languages.},
  eventtitle = {2021 36th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  isbn = {978-1-6654-0337-5},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/Q9YPFLAN/Kulkarni et al. - 2021 - Learning Highly Recursive Input Grammars.pdf}
}

@article{kumarPythonLanguageComparison2022,
  title = {Python versus {{C Language}}: {{A Comparison}}},
  author = {Kumar, Rohit and Chander, Subhash and Chahal, Mandeep},
  date = {2022},
  volume = {9},
  abstract = {C and python has been proven programming language in their respective domain. Python has been a newer language in comparison to C language and offers rich set of libraries and offers significant support for newer application areas such as machine learning, whereas c has a proven and benchmark language for system programming and real time application development. Ease of coding in one perceptive that differentiate these programming languages. Other perspective is the application areas addressed by these programming languages. On this research paper all these parameters including efficiency, portability, application domains etc. has been discussed in details and comparison has been done for the same. Poularity comparison of both the langauges hgas also been done in w.r.t. each other ans w.r.t other majorly used programming languages.},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/C8X9UFNM/Kumar et al. - 2022 - Python versus C Language A Comparison.pdf}
}

@online{lampinenCanLanguageModels2023,
  title = {Can Language Models Handle Recursively Nested Grammatical Structures? {{A}} Case Study on Comparing Models and Humans},
  shorttitle = {Can Language Models Handle Recursively Nested Grammatical Structures?},
  author = {Lampinen, Andrew Kyle},
  date = {2023-02-16},
  eprint = {2210.15303},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.15303},
  url = {http://arxiv.org/abs/2210.15303},
  urldate = {2025-10-16},
  abstract = {How should we compare the capabilities of language models (LMs) and humans? I draw inspiration from comparative psychology to highlight some challenges. In particular, I consider a case study: processing of recursively nested grammatical structures. Prior work suggests that LMs cannot handle these structures as reliably as humans can. However, the humans were provided with instructions and training, while the LMs were evaluated zero-shot. I therefore match the evaluation more closely. Providing large LMs with a simple prompt—substantially less content than the human training—allows the LMs to consistently outperform the human results, and even to extrapolate to more deeply nested conditions than were tested with humans. Further, reanalyzing the prior human data suggests that the humans may not perform above chance at the difficult structures initially. Thus, large LMs may indeed process recursively nested grammatical structures as reliably as humans. This case study highlights how discrepancies in the evaluation can confound comparisons of language models and humans. I therefore reflect on the broader challenge of comparing human and model capabilities, and highlight an important difference between evaluating cognitive models and foundation models.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/sulavmalla/Zotero/storage/PW4KMWUL/Lampinen - 2023 - Can language models handle recursively nested grammatical structures A case study on comparing mode.pdf}
}

@online{liIncrementalContextfreeGrammar2024,
  title = {Incremental {{Context-free Grammar Inference}} in {{Black Box Settings}}},
  author = {Li, Feifei and Chen, Xiao and Xiao, Xi and Sun, Xiaoyu and Chen, Chuan and Wang, Shaohua and Han, Jitao},
  date = {2024-09-20},
  eprint = {2408.16706},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.16706},
  url = {http://arxiv.org/abs/2408.16706},
  urldate = {2025-10-16},
  abstract = {Black-box context-free grammar inference presents a significant challenge in many practical settings due to limited access to example programs. The state-of-the-art methods, Arvada and Treevada, employ heuristic approaches to generalize grammar rules, initiating from flat parse trees and exploring diverse generalization sequences. We have observed that these approaches suffer from low quality and readability, primarily because they process entire example strings, adding to the complexity and substantially slowing down computations. To overcome these limitations, we propose a novel method that segments example strings into smaller units and incrementally infers the grammar. Our approach, named Kedavra, has demonstrated superior grammar quality (enhanced precision and recall), faster runtime, and improved readability through empirical comparison.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/Users/sulavmalla/Zotero/storage/YV8QVFYU/Li et al. - 2024 - Incremental Context-free Grammar Inference in Black Box Settings.pdf}
}

@article{mulikComparisonParsingTechniques,
  title = {Comparison of {{Parsing Techniques For Formal Languages}}},
  author = {Mulik, Sunanda and Shinde, Sheetal and Kapase, Smita},
  abstract = {A parser is one of the components in an interpreter or compiler, which checks for correct syntax and builds a data structure (often some kind of parse tree, abstract syntax tree or other hierarchical structure) implicit in the input tokens. Parsers may be programmed by hand or may be (semi-)automatically generated by a parser generating tool. Various techniques are available for parsing formal languages. The objective of this paper is to compare these techniques. The paper is organized in two sections. The first section does discuss about the parsing problem and process. In the second section we study and compare three parsing techniques theoretically .Finally the paper concludes with the suggestion of a new parsing technique.},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/I3TAQ8BU/Mulik et al. - Comparison of Parsing Techniques For Formal Languages.pdf}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  date = {2015-08-28},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  url = {https://www.science.org/doi/10.1126/science.aac4716},
  urldate = {2025-10-15},
  abstract = {Empirically analyzing empirical evidence                            One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts               et al.               describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.                                         Science               , this issue               10.1126/science.aac4716                        ,              A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.           ,                             INTRODUCTION               Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.                                         RATIONALE               There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.                                         RESULTS                                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and                 P                 values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (                 M                 r                 = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (                 M                 r                 = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (                 P                 {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.                                                        CONCLUSION                                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original                 P                 value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.                              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.                                                   Original study effect size versus replication effect size (correlation coefficients).                   Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.                                                                         ,              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/DI5GTHQY/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological science.pdf}
}

@article{schroderStaticInferenceRegular,
  title = {Static {{Inference}} of {{Regular Grammars}} for {{Ad Hoc Parsers}}},
  author = {Schröder, Michael and Cito, Jürgen and family=Wien, given=TU, given-i=TU},
  langid = {english},
  file = {/Users/sulavmalla/Zotero/storage/QGDRZJGL/Schröder et al. - Static Inference of Regular Grammars for Ad Hoc Parsers.pdf}
}

@inproceedings{shepperdReplicationStudiesConsidered2018,
  title = {Replication Studies Considered Harmful},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Software Engineering}}: {{New Ideas}} and {{Emerging Results}}},
  author = {Shepperd, Martin},
  date = {2018-05-27},
  eprint = {1802.04580},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {73--76},
  doi = {10.1145/3183399.3183423},
  url = {http://arxiv.org/abs/1802.04580},
  urldate = {2025-10-09},
  abstract = {Objective: To consider what is required for a replication study to confirm the original experiment and apply this understanding in software engineering. Method: Simulation is used to demonstrate why the prediction interval for confirmation can be surprisingly wide. This analysis is applied to three recent replications. Results: It is shown that because the prediction intervals are wide, almost all replications are confirmatory, so in that sense there is no ‘replication crisis’, however, the contributions to knowledge are negligible. Conclusion: Replicating empirical software engineering experiments, particularly if they are under-powered or under-reported, is a waste of scientific resources. By contrast, meta-analysis is strongly advocated so that all relevant experiments are combined to estimate the population effect.},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/sulavmalla/Zotero/storage/ZMXJGSI7/Shepperd - 2018 - Replication studies considered harmful.pdf}
}

@book{shiIntelligenceScience2021,
  title = {Intelligence {{Science}}},
  author = {Shi, Z},
  date = {2021},
  pagetotal = {215-266}
}
